{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install timm\n","!pip install mediapipe"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"6EasNuB-r6LI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import mediapipe as mp\n","from PIL import Image, ImageOps\n","import numpy as np\n","\n","mp_face_detection = mp.solutions.face_detection\n","mp_drawing = mp.solutions.drawing_utils\n","\n","def detect_and_crop_face(image):\n","    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n","        image_np = np.array(image)\n","        results = face_detection.process(image_np)\n","        if results.detections:\n","            detection = results.detections[0]\n","            bbox = detection.location_data.relative_bounding_box\n","            ih, iw, _ = image_np.shape\n","            xmin = int(bbox.xmin * iw)\n","            ymin = int(bbox.ymin * ih)\n","            width = int(bbox.width * iw)\n","            height = int(bbox.height * ih)\n","            xmax = xmin + width\n","            ymax = ymin + height\n","            face = image.crop((xmin, ymin, xmax, ymax))\n","            return face\n","        else:\n","            return -1\n","\n","\n"],"metadata":{"id":"JLnxaa0mr6LJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","path='/content/drive/MyDrive/TEST_DATA_SET'\n","anger_path=os.path.join(path,'anger')\n","happy_path=os.path.join(path,'happy')\n","panic_path=os.path.join(path,'panic')\n","sadness_path=os.path.join(path,'sadness')\n","label_path=os.path.join(path,'label (라벨링)')"],"metadata":{"id":"S0_O-0xyr6LJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","test_df=pd.DataFrame(columns=['img_path','label','gender','age'])\n","i=0\n","for label_name in os.listdir(label_path):\n","    with open(os.path.join(label_path,label_name),'r',encoding='cp949') as f:\n","        file=json.load(f)\n","        for v in file:\n","            if v['filename'].split('.')[-1]=='jpeg':\n","                continue\n","            if v['gender']=='남':\n","                gender='남자'\n","            else:\n","                gender='여자'\n","            if v['faceExp_uploader']=='분노':\n","                label='anger'\n","            elif v['faceExp_uploader']=='기쁨':\n","                label='happy'\n","            elif v['faceExp_uploader']=='당황':\n","                label='panic'\n","            elif v['faceExp_uploader']=='슬픔':\n","                label='sadness'\n","            test_df.loc[i]=[os.path.join(path,label,v['filename']),label,gender,v['age']]\n","            i+=1\n","test_df.sample(frac=1).reset_index(drop=True)"],"metadata":{"id":"n8qcGkZyr6LJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n","expression_test_transform = Compose([\n","    Resize((224, 224)),\n","    ToTensor(),\n","    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","])\n","age_test_transform=Compose([\n","    Resize((256,256)),\n","    ToTensor(),\n","    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","])\n","gender_test_transform=Compose([\n","    Resize((256,256)),\n","    ToTensor(),\n","    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","])"],"metadata":{"id":"fWSaTbn8r6LK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["expression_model = torch.load(\"/content/drive/MyDrive/best_model.pt\", map_location=device)\n","age_model = torch.load('/content/drive/MyDrive/swinv2_ages.pt', map_location=device)\n","gender_model = torch.load('/content/drive/MyDrive/swinv2_gender.pt', map_location=device)\n","expression_model.eval()\n","age_model.eval()\n","gender_model.eval()\n","expression_model.to(device)\n","age_model.to(device)\n","gender_model.to(device)"],"metadata":{"id":"ivD-LG4Xr6LK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, mean_absolute_error\n","label=['anger','happy','panic','sadness']\n","gender=['남자','여자']\n","expression_preds,expression_true_labels=[],[]\n","age_preds,age_true_labels=[],[]\n","gender_preds,gender_true_labels=[],[]\n","with torch.no_grad():\n","    for i in tqdm(range(len(test_df))):\n","        img_path=test_df.iloc[i]['img_path']\n","        img=Image.open(img_path).convert('RGB')\n","        img=ImageOps.exif_transpose(img)\n","        cropped_img=detect_and_crop_face(img)\n","        if cropped_img==-1:\n","            print(test_df.iloc[i]['img_path'])\n","            continue\n","        expression_img=expression_test_transform(cropped_img)\n","        age_img=age_test_transform(cropped_img)\n","        gender_img=gender_test_transform(cropped_img)\n","        expression_img=expression_img.unsqueeze(0).to(device)\n","        age_img=age_img.unsqueeze(0).to(device)\n","        gender_img=gender_img.unsqueeze(0).to(device)\n","        expression_pred=expression_model(expression_img)\n","        age_pred=age_model(age_img)\n","        gender_pred=gender_model(gender_img)\n","        expression_pred=label[expression_pred.argmax(1).detach().cpu().numpy().tolist()[0]]\n","        age_pred=age_pred.detach().cpu().item()\n","        gender_pred=gender[(torch.sigmoid(gender_pred) > 0.5).int().detach().cpu().numpy().tolist()[0][0]]\n","        expression_preds.append(expression_pred)\n","        expression_true_labels.append(test_df.iloc[i]['label'])\n","        age_preds.append(age_pred)\n","        age_true_labels.append(test_df.iloc[i]['age'])\n","        gender_preds.append(gender_pred)\n","        gender_true_labels.append(test_df.iloc[i]['gender'])\n"],"metadata":{"id":"EXKvC5Swr6LK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["expression_acc=accuracy_score(expression_true_labels,expression_preds)\n","age_mae = mean_absolute_error(age_true_labels, age_preds)\n","gender_acc=accuracy_score(gender_true_labels,gender_preds)"],"metadata":{"id":"17IYrLQsr6LK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'expression_acc:{expression_acc}')\n","print(f'age_mae:{age_mae}')\n","print(f'gender_acc:{gender_acc}')"],"metadata":{"id":"KNuIzbT8r6LK"},"execution_count":null,"outputs":[]}]}